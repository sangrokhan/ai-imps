ğŸ“‘ Implementation Guide: Dynamically Expandable Networks (DEN)
ì´ ë¬¸ì„œëŠ” íƒœìŠ¤í¬ë³„ ê°€ì¤‘ì¹˜ ê´€ë¦¬, ê°€ë¡œ í™•ì¥, ê·¸ë¦¬ê³  ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•œ Split & Duplication ê¸°ëŠ¥ì„ í¬í•¨í•œ DENì˜ PyTorch êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.
1. ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™
 * Heterogeneous Task Handling: nn.ModuleDictë¥¼ í™œìš©í•˜ì—¬ íƒœìŠ¤í¬ë§ˆë‹¤ ë‹¤ë¥¸ ì…ë ¥(D_{in}) ë° ì¶œë ¥(D_{out}) ì°¨ì›ì„ ì²˜ë¦¬í•˜ëŠ” ë…ë¦½ì ì¸ Headì™€ Tailì„ ìƒì„±í•©ë‹ˆë‹¤.
 * Modular Training: í•™ìŠµ ë¡œì§ì„ ëª¨ë¸ í´ë˜ìŠ¤ì™€ ë¶„ë¦¬í•˜ì—¬ ë°ì´í„°, ì†ì‹¤ í•¨ìˆ˜, í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìœ ì—°í•˜ê²Œ ì£¼ì…í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í•©ë‹ˆë‹¤.
 * Dynamic Weight Management: ê°€ì¤‘ì¹˜ ì €ì¥ ê²½ë¡œë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•˜ë©°, íƒœìŠ¤í¬ë³„ë¡œ ë³„ë„ì˜ ê°€ì¤‘ì¹˜ íŒŒì¼(.pt)ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
2. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (Config)
í•™ìŠµ ì‹œ ì¸ìë¡œ ì „ë‹¬í•˜ê±°ë‚˜ ë³„ë„ì˜ YAML/JSONìœ¼ë¡œ ê´€ë¦¬í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° í•­ëª©ì…ë‹ˆë‹¤.
| í•­ëª© | ì„¤ëª… | ê¶Œì¥ ì´ˆê¸°ê°’ |
|---|---|---|
| l1_lambda | Selective Retraining ì‹œ Sparse ì—°ê²°ì„ ìœ ë„í•˜ëŠ” L_1 ê³„ìˆ˜ | 10^{-3} \sim 10^{-4} |
| expansion_threshold | ë„¤íŠ¸ì›Œí¬ í™•ì¥ì„ ê²°ì •í•˜ëŠ” ì†ì‹¤(Loss) ì„ê³„ê°’ | ì‚¬ìš©ì ì§€ì • |
| drift_threshold (\epsilon) | ë‰´ëŸ° ë¶„í• (Split)ì„ ê²°ì •í•˜ëŠ” ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰ ì„ê³„ê°’ | 0.01 \sim 0.1 |
| new_nodes_k | í™•ì¥ ì‹œ ë ˆì´ì–´ë‹¹ ì¶”ê°€í•  ë‰´ëŸ° ìˆ˜ | 10 \sim 20 |
3. ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ
[Step 1] ì¸í”„ë¼ ë° ê°€ì¤‘ì¹˜ ê´€ë¦¬ ë¡œì§
ê°€ì¤‘ì¹˜ ì €ì¥ ìœ„ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •í•˜ê³ , íƒœìŠ¤í¬ë³„ ì…ì¶œë ¥ ì¸µì„ ìƒì„±í•©ë‹ˆë‹¤.
import torch
import torch.nn as nn
import os

class DEN(nn.Module):
    def __init__(self, hidden_dims):
        super().__init__()
        self.hidden_dims = hidden_dims # List of hidden units per layer
        self.shared_layers = nn.ModuleList() # Hidden layers
        self.task_inputs = nn.ModuleDict()  # Task-specific input heads
        self.task_outputs = nn.ModuleDict() # Task-specific output tails

    def add_task_layer(self, task_id, in_dim, out_dim):
        """íƒœìŠ¤í¬ë³„ ì…ì¶œë ¥ ì¸µ ë™ì  ìƒì„±"""
        if task_id not in self.task_inputs:
            self.task_inputs[task_id] = nn.Linear(in_dim, self.hidden_dims[0])
            self.task_outputs[task_id] = nn.Linear(self.hidden_dims[-1], out_dim)

    def save_weights(self, task_id, save_dir):
        """ì§€ì •ëœ ê²½ë¡œì— íƒœìŠ¤í¬ë³„ ê°€ì¤‘ì¹˜ ì €ì¥"""
        os.makedirs(save_dir, exist_ok=True)
        path = os.path.join(save_dir, f"{task_id}.pt")
        torch.save({
            'state_dict': self.state_dict(),
            'hidden_dims': self.hidden_dims
        }, path)

    def load_weights(self, task_id, save_dir):
        path = os.path.join(save_dir, f"{task_id}.pt")
        if os.path.exists(path):
            checkpoint = torch.load(path)
            # ë ˆì´ì–´ í¬ê¸°ê°€ ë³€í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ strict=False ì‚¬ìš© ë˜ëŠ” êµ¬ì¡° ì¬êµ¬ì„± í•„ìš”
            self.load_state_dict(checkpoint['state_dict'], strict=False)

[Step 2] ë³„ë„ í•™ìŠµ í•¨ìˆ˜ ë° Selective Retraining
ëª¨ë¸ ì™¸ë¶€ì—ì„œ í•™ìŠµì„ ì œì–´í•˜ë©° L_1 ì •ê·œí™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
def train_den_step(model, train_loader, criterion, optimizer_fn, config):
    """
    model: DEN ëª¨ë¸ ê°ì²´
    criterion: Loss í•¨ìˆ˜ (ì˜ˆ: nn.CrossEntropyLoss)
    optimizer_fn: ì˜µí‹°ë§ˆì´ì € ìƒì„± íŒ©í† ë¦¬ (ì˜ˆ: lambda p: torch.optim.Adam(p, lr=1e-3))
    config: í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
    """
    model.train()
    # 1. Selective Retrainingì„ ìœ„í•œ íŒŒë¼ë¯¸í„° í•„í„°ë§ (í˜„ì¬ íƒœìŠ¤í¬ ê´€ë ¨ë§Œ)
    params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = optimizer_fn(params)

    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        
        # ê¸°ë³¸ Loss + L1 Regularization
        l1_reg = sum(p.abs().sum() for p in model.parameters())
        loss = criterion(output, target) + config['l1_lambda'] * l1_reg
        
        loss.backward()
        optimizer.step()
    
    return loss.item()

[Step 3] Dynamic Expansion (ê°€ë¡œ í™•ì¥)
ëª¨ë“  íˆë“  ë ˆì´ì–´ì˜ ì°¨ì›ì„ ê°€ë¡œë¡œ í™•ì¥í•©ë‹ˆë‹¤.
def expand_network(model, k):
    """ëª¨ë“  íˆë“  ë ˆì´ì–´ì— kê°œì˜ ë‰´ëŸ° ì¶”ê°€"""
    with torch.no_grad():
        for i, layer in enumerate(model.shared_layers):
            # 1. í˜„ì¬ ë ˆì´ì–´ì˜ Output ì°¨ì› í™•ì¥
            # 2. ë‹¤ìŒ ë ˆì´ì–´ì˜ Input ì°¨ì› í™•ì¥ (ì—°ì‡„ ì ìš©)
            # Linear ë ˆì´ì–´ì˜ weight íŒŒë¼ë¯¸í„°ë¥¼ torch.catìœ¼ë¡œ í™•ì¥ëœ ìƒˆ íŒŒë¼ë¯¸í„°ë¡œ êµì²´
            pass 
    # í™•ì¥ í›„ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë³€í•˜ë¯€ë¡œ ì˜µí‹°ë§ˆì´ì €ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

[Step 4] Split & Duplication (ì „ì²´ ë ˆì´ì–´ ì ìš©)
í•™ìŠµ ì „í›„ ê°€ì¤‘ì¹˜ë¥¼ ë¹„êµí•˜ì—¬ ì§€ì‹ ì†ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
 * í•™ìŠµ ì „ old_state = model.state_dict()ë¥¼ ë³µì‚¬í•©ë‹ˆë‹¤.
 * í•™ìŠµ í›„ ê° ë‰´ëŸ°(Row)ë³„ë¡œ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
 * ì„ê³„ê°’(\epsilon)ì„ ë„˜ëŠ” ë‰´ëŸ° ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ expand_networkì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ í•´ë‹¹ ë‰´ëŸ°ë§Œ ë³µì œí•˜ì—¬ ë ˆì´ì–´ í¬ê¸°ë¥¼ í‚¤ì›ë‹ˆë‹¤.
 * ë³µì œëœ ë‰´ëŸ° ì¤‘ í•˜ë‚˜ì—ëŠ” old_stateì˜ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•˜ì—¬ ì´ì „ ì§€ì‹ì„ ë³´ì¡´í•©ë‹ˆë‹¤.
4. êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­
 * Weight Mapping: ë ˆì´ì–´ê°€ í™•ì¥ë˜ë©´ ì´ì „ íƒœìŠ¤í¬ì˜ ë‰´ëŸ° ì¸ë±ìŠ¤ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° íƒœìŠ¤í¬ê°€ ì–´ë–¤ ë‰´ëŸ° ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ë§µí•‘ í…Œì´ë¸”(dict)ì„ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì €ì¥í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.
 * Zero-Grad Masking: Selective Retraining ì‹œ ì„ íƒë˜ì§€ ì•Šì€ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ê°€ ë³€í•˜ì§€ ì•Šë„ë¡ gradë¥¼ ê°•ì œë¡œ 0ìœ¼ë¡œ ë§Œë“œëŠ” ë§ˆìŠ¤í‚¹ ë¡œì§ì„ optimizer.step() ì§ì „ì— ì¶”ê°€í•˜ì„¸ìš”.
